<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>AI-10: The nihilistic craving for x-risk</title>
</head>
<body>
  <h1>AI-10: The nihilistic craving for x-risk</h1>
  <p><em>2024-04-13T06:11:33.598Z</em></p>
  <p><strong>Original:</strong> <a href="https://discourse.suttacentral.net/t/ai-10-the-nihilistic-craving-for-x-risk/33476">https://discourse.suttacentral.net/t/ai-10-the-nihilistic-craving-for-x-risk/33476</a></p>
  <p>
 Existential risk is sexy.
 <a href="https://archive.md/j4lVw">
  Like Oppenheimer
 </a>
 , tech gurus can become Death, destroyer of worlds. The AI world is full of people who on the one hand tell us that their chatbot is the greatest thing ever invented, key to the mysteries of consciousness itself, and in the next breath warn us that it may well kill every human being alive.
</p>
<p>
 Sam Altman has
 <a href="https://www.truthdig.com/articles/the-madness-of-the-race-to-build-artificial-general-intelligence/">
  some interesting things to say
 </a>
 about the threat AI poses to the very survival of humanity.
</p>
<blockquote>
 <p>
  the bad case — and I think this is important to say — is, like, lights out for all of us.
 </p>
</blockquote>
<p>
 As for the next sentence, each time I read it I am newly amazed. It’s possibly one of the most incredible things a human has ever said.
</p>
<blockquote>
 <p>
  <a href="https://twitter.com/liron/status/1760056584519213070">
   I think AI will… most likely sort of lead to the end of the world
  </a>
  , but in the meantime there will be great companies created with serious machine learning
 </p>
</blockquote>
<p>
 That a person could even put these ideas together and let them pass their mouth is incomprehensible to me. But anyway, he is clear, AI is not just
 <em>
  a
 </em>
 threat, it is
 <em>
  the
 </em>
 threat.
</p>
<blockquote>
 <p>
  development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity.
 </p>
</blockquote>
<p>
 In a formal statement on the OpenAI website, he says:
</p>
<blockquote>
 <p>
  Some people in the AI field think the risks of AGI (and successor systems) are fictitious; we would be delighted if they turn out to be right, but we are going to operate as if these risks are existential.
 </p>
</blockquote>
<p>
 His views are normal in the AI community.
</p>
<blockquote>
 <p>
  <strong>
   Dario Amodei, CEO of Anthropic
  </strong>
  : “I think at the extreme end is the … fear that an AGI could destroy humanity. I can’t see any reason in principle why that couldn’t happen.”
 </p>
</blockquote>
<blockquote>
 <p>
  <strong>
   Elon Musk, founder and CEO of xAI:
  </strong>
  “one of the biggest risks to the future of civilization is AI”.
 </p>
</blockquote>
<blockquote>
 <p>
  <strong>
   Sundar Pichai, the
   <a href="https://www.notablebrahmins.com/sundar-pichai/">
    brahmin
   </a>
   CEO of Google:
  </strong>
  advanced AI “can be very harmful if deployed wrongly,” and that with respect to safety issues, “we don’t have all the answers there yet, and the technology is moving fast. … So does that keep me up at night? Absolutely.”
 </p>
</blockquote>
<blockquote>
 <p>
  <strong>
   Shane Legg, co-founder of DeepMInd:
  </strong>
  “We have no idea how to solve this problem.”
 </p>
</blockquote>
<p>
 Since everything that is happening today is unprecedented, there is definitely a chance that AI will destroy humanity. But I believe this is the first time the developers of a technology have flirted so openly with nihilistic desire. They sell us anxiety and fear because they know we’re addicted.
</p>
<p>
 But I think it’s more than that. It’s not just a cynical marketing ploy, nor is it rational. It’s nihilism. I think they
 <em>
  want
 </em>
 humanity to die. I think there’s a deep level hatred of themselves, manifesting as hatred of humanity, that at some unconscious level is fueling this inchoate need to propel ourselves to a future where we no longer cause any problems because we don’t exist.
</p>
<p>
 In Buddhism we call this
 <em>
  vibhavataṇhā
 </em>
 , the craving for annihilation. It manifests as addictive and self-harming behavior, and in extreme forms as suicide. But these guys are coyly flirting with suicide of the whole human race. Look, if that’s their thing, fine. People are messed up. All I’m saying is that sane people need to stop them.
</p>
<p>
 It’s impossible to guess the actual risk and foolish to try. I personally
 <a href="https://www.truthdig.com/articles/does-agi-really-threaten-the-survival-of-the-species/">
  agree with Émile P. Torres
 </a>
 that the risk of extinction is overrated. Its difficult to imagine a genuine scenario leading to extinction. That, however, doesn’t change the fact that, as the OpenAI website says, “a misaligned superintelligent AGI could cause grievous harm to the world”.
</p>
<p>
 The real risk is the actual things that are destroying the world, primarily climate change. AI distracts us from the real problems, offers no genuine solutions, and in doing so consumes a vast amount of energy, both physical and mental.
</p>
<p>
 An
 <a href="https://twitter.com/xriskology/status/1763328477477183878">
  OpenAI employee tweeted
 </a>
 that since everything is accelerating, just relax and worry about “stupid mortal things” like spending time with your families while AGI becomes a reality: “I don’t feel any control everyone else certainly shouldn’t”.
</p>
<p>
 Has any product ever been sold with such reckless nihilism? Even with nuclear weapons, they would tell us they could destroy the world, but at the same time reassure us that they would try their best not to use them. How is it
 <em>
  vaguely legal
 </em>
 to advertise your product as a world-destroyer and then just put it into everyone’s computers?
</p>
</body>
</html>
